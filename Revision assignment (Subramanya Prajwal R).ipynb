{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1aKWKl4LFer"
   },
   "source": [
    "# Keyword Detection on Websites\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t74wPcuOLaVW"
   },
   "source": [
    "## Assignment\n",
    "Your task is to create an algorithm, that takes html page as input and infers if the page contains the information about cancer tumorboard or not. What is a tumor board? Tumor Board is a consilium of doctors (usually from different disciplines) discussing cancer cases in their departments. If you want to know more please read this article.\n",
    "\n",
    "The expected result is a CSV file for test data with columns [doc_id and prediction].\n",
    "\n",
    "Bonus: if you would like to go the extra mile in this task try to identify tumor board types interdisciplinary, breast, and any third type of tumor board up to you. For these tumor boards please try to identify their schedule: Day (e.g. Friday), frequency (e.g. weekly, bi-weekly, monthly), and time when they start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VDsPJHuLert"
   },
   "source": [
    "## Data Description\n",
    "You have train.csv and test.csv files and folder with corresponding .html files.\n",
    "\n",
    "Files:\n",
    "\n",
    "train.csv contains next columns: url, doc_id and label\n",
    "test.csv contains next columns: url and doc_id\n",
    "htmls contains files with names {doc_id}.html\n",
    "keyword2tumor_type.csv contains useful keywords for types of tumorboards\n",
    "Description of tumor board labels:\n",
    "\n",
    "1 (no evidence): tumor boards are not mentioned on the page\n",
    "2 (medium confidence): tumor boards are mentioned, but the page is not completely dedicated to tumor board description\n",
    "3 (high confidence): page is completely dedicated to the description of tumor board types and dates\n",
    "You are asked to prepare a model using htmls, referred to in train.csv, and make predictions for htmls from test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYlqDlezLi6C"
   },
   "source": [
    "## Practicalities\n",
    "You should prepare a Jupyter Notebook with the code that you used for making the predictions and the following documentation:\n",
    "\n",
    "How did you decide to handle this amount of data?\n",
    "How did you decide to do feature engineering?\n",
    "How did you decide which models to try (if you decide to train any models)?\n",
    "How did you perform validation of your model?\n",
    "What metrics did you measure?\n",
    "How do you expect your model to perform on test data (in terms of your metrics)?\n",
    "How fast will your algorithm performs and how could you improve its performance if you would have more time?\n",
    "How do you think you would be able to improve your algorithm if you would have more data?\n",
    "What potential issues do you see with your algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNVf9nfELm8h"
   },
   "source": [
    "## Tips\n",
    "to extract clean text from the page you can use BeautifulSoup module like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzZZ1avgLsxh"
   },
   "source": [
    "from bs import BeautifulSoup\n",
    "\n",
    "content = read_html()\n",
    "\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "clean_text = soup.get_text(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9WAtI_zL0Xa"
   },
   "source": [
    "\n",
    "## If you decide that you don't need, for example, tags <p> in your document you can do this:##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQbiQRdYMA57"
   },
   "source": [
    "from bs import BeautifulSoup\n",
    "\n",
    "content = read_html()\n",
    "\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "for tag in soup.find_all('p'):\n",
    "    tag.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajUGGOYbNVrl"
   },
   "source": [
    "#### To download the dataset <a href=\"https://drive.google.com/drive/folders/1Qs2fLj9HmAzx2YGKmqkePCa1Acs5JY3Z?usp=sharing\"> Click here </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subramanya\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  doc_id  label  \\\n",
      "0  http://elbe-elster-klinikum.de/fachbereiche/ch...       1      1   \n",
      "1  http://klinikum-bayreuth.de/einrichtungen/zent...       3      3   \n",
      "2  http://klinikum-braunschweig.de/info.php/?id_o...       4      1   \n",
      "3  http://klinikum-braunschweig.de/info.php/?id_o...       5      1   \n",
      "4  http://klinikum-braunschweig.de/zuweiser/tumor...       6      3   \n",
      "\n",
      "                                                text  \n",
      "0  \\n \\n \\n \\n \\n \\n \\n \\n Elbe-Elster Klinikum -...  \n",
      "1  \\n \\n \\n \\n \\n \\n \\n \\n Onkologisches Zentrum ...  \n",
      "2  \\n \\n \\n Zentrum - Sozialpädiatrisches Zentrum...  \n",
      "3  \\n \\n \\n Leistung - Spezielle Unterstützung be...  \n",
      "4  \\n \\n \\n Zuweiser - Tumorkonferenzen - Tumorko...  \n",
      "                                                 url  doc_id  \\\n",
      "0  http://chirurgie-goettingen.de/medizinische-ve...       0   \n",
      "1  http://evkb.de/kliniken-zentren/chirurgie/allg...       2   \n",
      "2  http://krebszentrum.kreiskliniken-reutlingen.d...       7   \n",
      "3  http://marienhospital-buer.de/mhb-av-chirurgie...      15   \n",
      "4  http://marienhospital-buer.de/mhb-av-chirurgie...      16   \n",
      "\n",
      "                                                text  \n",
      "0  \\n \\n \\n \\n \\n \\n \\n Bauchspeicheldrüse | Klin...  \n",
      "1  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\...  \n",
      "2  \\n \\n \\n \\n Brustzentrum Reutlingen: Behandlun...  \n",
      "3  \\n \\n \\n \\n \\n \\n \\n Leistungsspektrum: Sankt ...  \n",
      "4  \\n \\n \\n \\n \\n \\n \\n Leistungsspektrum: Sankt ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load train and test datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Function to read and clean HTML content with encoding handling\n",
    "def read_html(doc_id, folder='htmls'):\n",
    "    file_path = os.path.join(folder, f\"{doc_id}.html\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                content = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"File encoding issue: {file_path}\")\n",
    "            content = \"\"\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        content = \"\"\n",
    "    \n",
    "    if content:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        clean_text = soup.get_text(' ')\n",
    "    else:\n",
    "        clean_text = \"\"\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Extract and clean text for training data\n",
    "train_df['text'] = train_df['doc_id'].apply(read_html)\n",
    "test_df['text'] = test_df['doc_id'].apply(read_html)\n",
    "\n",
    "# Display the first few rows to ensure the data is loaded correctly\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (80, 2000)\n",
      "X_val shape: (20, 2000)\n",
      "y_train shape: (80,)\n",
      "y_val shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text using TF-IDF with ngrams\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=2000, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(train_df['text'])\n",
    "y = train_df['label']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting matrices\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Precision: 0.425\n",
      "Recall: 0.5\n",
      "F1 Score: 0.39920634920634923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subramanya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Retrain a Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the retrained model\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Score: 0.7125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45\n",
      "Precision: 0.23684210526315788\n",
      "Recall: 0.45\n",
      "F1 Score: 0.3103448275862069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subramanya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Retrain a Random Forest model with the best parameters\n",
    "best_model = RandomForestClassifier(random_state=42, max_depth=None, min_samples_split=2, n_estimators=200)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the retrained model\n",
    "y_pred = best_model.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.4\n",
      "SVM Precision: 0.23529411764705882\n",
      "SVM Recall: 0.4\n",
      "SVM F1 Score: 0.29629629629629634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subramanya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the SVM model\n",
    "y_pred_svm = svm_model.predict(X_val)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_val, y_pred_svm))\n",
    "print(\"SVM Precision:\", precision_score(y_val, y_pred_svm, average='weighted'))\n",
    "print(\"SVM Recall:\", recall_score(y_val, y_pred_svm, average='weighted'))\n",
    "print(\"SVM F1 Score:\", f1_score(y_val, y_pred_svm, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.5\n",
      "Gradient Boosting Precision: 0.4076923076923077\n",
      "Gradient Boosting Recall: 0.5\n",
      "Gradient Boosting F1 Score: 0.4478260869565217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subramanya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the Gradient Boosting model\n",
    "y_pred_gb = gb_model.predict(X_val)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_val, y_pred_gb))\n",
    "print(\"Gradient Boosting Precision:\", precision_score(y_val, y_pred_gb, average='weighted'))\n",
    "print(\"Gradient Boosting Recall:\", recall_score(y_val, y_pred_gb, average='weighted'))\n",
    "print(\"Gradient Boosting F1 Score:\", f1_score(y_val, y_pred_gb, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Evaluation:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.4076923076923077\n",
      "Recall: 0.5\n",
      "F1 Score: 0.4478260869565217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subramanya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_gb = gb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_gb = accuracy_score(y_val, y_pred_gb)\n",
    "precision_gb = precision_score(y_val, y_pred_gb, average='weighted')\n",
    "recall_gb = recall_score(y_val, y_pred_gb, average='weighted')\n",
    "f1_gb = f1_score(y_val, y_pred_gb, average='weighted')\n",
    "\n",
    "print(\"Gradient Boosting Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_gb)\n",
    "print(\"Precision:\", precision_gb)\n",
    "print(\"Recall:\", recall_gb)\n",
    "print(\"F1 Score:\", f1_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Evaluation:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.425\n",
      "Recall: 0.5\n",
      "F1 Score: 0.39920634920634923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subramanya\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_val, y_pred_rf)\n",
    "precision_rf = precision_score(y_val, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_val, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_val, y_pred_rf, average='weighted')\n",
    "\n",
    "print(\"Random Forest Classifier Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  I decided to handle the amount of data by first loading the provided train and test datasets into memory.\n",
    "Since the size of the dataset was manageable, I chose to load the entire dataset at once rather than using techniques\n",
    "like data batching or streaming. Additionally, I ensured to split the dataset into training and validation sets to evaluate\n",
    "the model's performance effectively.\n",
    "\n",
    "\n",
    "2.  For feature engineering, I opted to utilize the TF-IDF vectorization technique to convert the text data from HTML pages into numerical features. TF-IDF was chosen because it effectively captures the importance of words within each document while accounting for their frequency across the entire corpus. I also considered experimenting with keyword extraction techniques to identify relevant terms related to tumor board\n",
    "\n",
    "\n",
    "3.  I decided to try three different models for classification: Support Vector Machines (SVM), Gradient Boosting, and Random Forest. I chose these models based on their versatility and effectiveness in handling classification tasks with text data. Additionally, these models offer a good balance between interpretability and performance, which was important for this task.\n",
    "\n",
    "\n",
    "4.  To validate the model, I split the training data into training and validation sets using a stratified approach to ensure class balance. I then trained each model on the training set and evaluated its performance on the validation set. Additionally, I used k-fold cross-validation to assess the models' generalization performance and mitigate overfitting.\n",
    "\n",
    "\n",
    "5.  I measured several classification metrics to evaluate the models' performance, including accuracy, precision, recall, and F1 score. These metrics provide insights into different aspects of the model's performance, such as overall correctness, positive predictive value, sensitivity, and balance between precision and recall.\n",
    "\n",
    "\n",
    "6. Based on the evaluation metrics obtained on the validation set, I expect the model to perform reasonably well on the test data. However, I anticipate some variation in performance due to differences in the distribution of data between the training/validation and test sets. Nonetheless, I believe the chosen models have the potential to generalize well to unseen data\n",
    "\n",
    "\n",
    "7. The algorithms performed reasonably fast on the provided dataset, but their performance may vary depending on the complexity of the data and computational resources available. but i tried somethings but couldnt get accuracy i am trying but deadline has come close so i have to submit what i have done so far, but after submiting i have to try get more accuracy and see where i missed things\n",
    "\n",
    "\n",
    "8.  With more data, I could train the models on a larger and more diverse dataset, which may lead to improved generalization and performance.\n",
    "\n",
    "\n",
    "9.  One potential issue with the algorithm is its sensitivity to the quality and representativeness of the training data. If the dataset is biased or contains noisy or irrelevant information, it could affect the model's performance negatively.\n",
    "but i need more practice on these kind of problems so i will work hard as much as possible. Regularization techniques and careful model selection can help mitigate these issues. so i will see what i can do after submiting this. Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
